{"cells":[{"cell_type":"markdown","id":"9a33e3bc","metadata":{"id":"9a33e3bc"},"source":["## SPAM DETECTION"]},{"cell_type":"markdown","id":"ecee1657","metadata":{"id":"ecee1657"},"source":["# Spam Detection & Email Analysis\n","\n","## Contesto\n","**ProfessionAI** ha richiesto lo sviluppo di una libreria per l'analisi delle email ricevute,\n","con focus sull'identificazione e l'analisi delle email di tipo SPAM.\n","\n","## Obiettivi\n","Il progetto si articola in 4 task principali:\n","\n","- **Classificazione SPAM** ‚Äî addestrare un modello per identificare automaticamente le email spam\n","- **Topic Modeling** ‚Äî individuare i topic principali tra le email SPAM\n","- **Distanza Semantica** ‚Äî calcolare la distanza tra i topic per misurarne l'eterogeneit√†\n","- **NER (Named Entity Recognition)** ‚Äî estrarre le organizzazioni dalle email NON SPAM\n","\n","## Dataset\n","Dataset fornito da ProfessionAI contenente email etichettate come SPAM e non SPAM.\n","\n","##  Tecnologie utilizzate\n","- **Python** (Pandas, Scikit-learn, NLTK/SpaCy)\n","- **Google Colab**"]},{"cell_type":"code","execution_count":19,"id":"bfa987af","metadata":{"id":"bfa987af","executionInfo":{"status":"ok","timestamp":1771926956758,"user_tz":-60,"elapsed":11,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":20,"id":"34fcb190","metadata":{"id":"34fcb190","executionInfo":{"status":"ok","timestamp":1771926957220,"user_tz":-60,"elapsed":460,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}}},"outputs":[],"source":["url = \"https://raw.githubusercontent.com/EliaVenturini/professionai-projects/main/spam_dataset.csv\"\n","dataset = pd.read_csv(url)"]},{"cell_type":"markdown","source":["**Importazione e configurazione delle librerie**\n","Ho iniziato importando le librerie necessarie per l‚Äôelaborazione del testo, tra cui string, spacy, nltk, re, pandas, numpy, gensim e sklearn. Successivamente:\n","\n","Ho caricato le stopwords in inglese utilizzando nltk.\n","Ho caricato il modello di word embeddings GloVe tramite gensim, per rappresentare le parole come vettori.\n","Ho inizializzato spaCy per poter gestire l‚Äôanalisi linguistica e la lemmatizzazione del testo.\n","Definizione delle funzioni principali\n","\n","- **Ho creato due funzioni chiave per la pulizia e la trasformazione del testo:**\n","\n","  - **data_cleaner(sentence)**: Questa funzione ha convertito il testo in minuscolo, rimosso la punteggiatura (sostituendola con spazi), applicato la lemmatizzazione per ridurre le parole alla loro forma base, eliminato le stopwords e infine rimosso i numeri dal testo.\n","\n","  - **avg_vector(sentence)**: Questa funzione calcolava il vettore medio della frase. Per ogni parola nella frase, se era presente nel modello GloVe, il suo vettore veniva sommato al vettore totale; altrimenti, veniva conteggiata come ‚Äúnon trovata‚Äù. Se nessuna parola era presente nel modello, la funzione restituiva un vettore di zeri; altrimenti, restituiva il vettore medio calcolato.\n","Caricamento e suddivisione del dataset\n","\n","- **Ho caricato il dataset spam_dataset.csv.**\n","Dopo aver estratto la colonna label per ottenere le etichette SPAM e non-SPAM, ho suddiviso il dataset in un set di training (80%) e uno di test (20%) utilizzando train_test_split.\n","\n","- **Pre-processing del testo nel dataset**\n","Per pulire i dati, ho applicato la funzione data_cleaner ai testi sia del set di training che di quello di test. Dopodich√©, ho convertito il testo pulito in vettori numerici, utilizzando la funzione avg_vector per trasformare ogni frase nei set di training e test.\n","\n","- **Addestramento del classificatore MLP**\n","Ho configurato e addestrato un Multi-Layer Perceptron (MLP) con:\n","\n"," - Funzione di attivazione logistica\n"," - Un singolo strato nascosto con 100 neuroni\n"," - Metodo di ottimizzazione Adam\n"," - Tolleranza impostata a 0.005\n","\n","Ho addestrato questo modello sui vettori di testo del set di training e sulle rispettive etichette di classificazione.\n","\n","- **Estrazione delle organizzazioni nelle email non-SPAM**\n","\n","Per identificare le organizzazioni citate nelle email non-SPAM:\n","\n","Ho definito la funzione extract_organizations(text), che utilizzava spaCy per identificare le entit√† nominate nella frase e restituiva solo le entit√† con etichetta ‚ÄúORG‚Äù (organizzazioni).\n","Ho quindi filtrato le email non-SPAM (etichettate come ‚Äúham‚Äù), applicato la pulizia del testo con data_cleaner, e utilizzato extract_organizations per estrarre e salvare le organizzazioni menzionate in queste email."],"metadata":{"id":"ex7NG3OKrqNg"},"id":"ex7NG3OKrqNg"},{"cell_type":"markdown","source":["DATA CLEANING"],"metadata":{"id":"KMgTqh3bqk3L"},"id":"KMgTqh3bqk3L"},{"cell_type":"code","source":["import string  # Per gestire la punteggiatura\n","import spacy   # Per l'analisi del linguaggio naturale (lemmatizzazione)\n","import nltk    # Libreria per l'elaborazione del linguaggio naturale\n","from nltk.corpus import stopwords  # Importa le stopwords da nltk\n","import re      # Libreria per le espressioni regolari (regex)\n","\n","nltk.download('stopwords') #dataset delle stopwords\n","english_stopwords = stopwords.words('english') #definisco le stopwords in inglese\n","nlp = spacy.load('en_core_web_sm') #carico il modello di Spacy per l'analisi del linguaggio (include lemmatizzazione)\n","punctuation = set(string.punctuation) #definisco la punteggiatura come un insieme\n","\n","def data_cleaner(sentence):\n","    sentence = sentence.lower() #converti la frase in minuscolo per un'elaborazione coerente\n","    for c in string.punctuation:\n","      sentence=sentence.replace(c,\" \") #rimuovo la punteggiatura sostituendola con spazi\n","    document = nlp(sentence) #analizzo la frase con Spacy per applicare la lemmatizzazione\n","    sentence = \" \".join(token.lemma_ for token in document) #lemmatizzo i token (restituisce la forma base di ogni parola)\n","    sentence = \" \".join(word for word in sentence.split() if word not in english_stopwords) #rimuovo le stopwords (parole comuni come 'and', 'the')\n","    sentence = re.sub(\"\\d\", \"\", sentence) #uso espressioni regolari per rimuovere i numeri dalla frase\n","    return sentence #ritorna la stringa\n"],"metadata":{"id":"DFniiAogqnUP","executionInfo":{"status":"ok","timestamp":1771926960799,"user_tz":-60,"elapsed":3578,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"72aa5658-3e67-4fa0-f2c5-e64d3e0526d2"},"id":"DFniiAogqnUP","execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:19: SyntaxWarning: invalid escape sequence '\\d'\n","<>:19: SyntaxWarning: invalid escape sequence '\\d'\n","/tmp/ipython-input-2867601852.py:19: SyntaxWarning: invalid escape sequence '\\d'\n","  sentence = re.sub(\"\\d\", \"\", sentence) #uso espressioni regolari per rimuovere i numeri dalla frase\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","!pip install gensim\n","from gensim.models import Word2Vec\n","import gensim.downloader\n","\n","glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"],"metadata":{"id":"WjyTAuzjr9Zj","executionInfo":{"status":"ok","timestamp":1771927164783,"user_tz":-60,"elapsed":203959,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25f0d4f0-afdc-4b20-a300-c8cd2db177b9"},"id":"WjyTAuzjr9Zj","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n"]}]},{"cell_type":"markdown","source":["CALCOLO DEL VETTORE MEDIO DELLE FRASI"],"metadata":{"id":"sLrHm0N2WYU9"},"id":"sLrHm0N2WYU9"},{"cell_type":"code","source":["import numpy as np\n","\n","#Funzione che calcola il vettore medio di una frase\n","def avg_vector(sentence):\n","    to_remove = 0 #Inizializzazione del contatore delle parole non trovate nei vettori GloVe\n","    vector = np.zeros(300)  #Inizializzione di un vettore di zeri di dimensione 300 (la dimensione dei vettori GloVe)\n","\n","    # Itera su ogni parola nella frase (sentence √® una lista di parole)\n","    for word in sentence:\n","        # Se la parola esiste nei vettori GloVe (controlla se √® presente in 'glove_vectors')\n","        if word in glove_vectors.key_to_index.keys():\n","            # Somma il vettore della parola al vettore accumulato\n","            vector += glove_vectors.get_vector(word)\n","        else:\n","            # Se la parola non √® presente nei vettori GloVe, aumenta il contatore di parole da rimuovere\n","            to_remove += 1\n","\n","    # Se tutte le parole non sono presenti nei vettori GloVe, restituisce un vettore di zeri\n","    if len(sentence) == to_remove:\n","        return np.zeros(300)  # Restituisce un vettore di zeri\n","\n","    #Restituisce il vettore medio dividendo il vettore accumulato per il numero di parole valide (non rimosse)\n","    return vector / (len(sentence) - to_remove)\n"],"metadata":{"id":"aPPyX_c5t9Vh","executionInfo":{"status":"ok","timestamp":1771927164792,"user_tz":-60,"elapsed":4,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}}},"id":"aPPyX_c5t9Vh","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["CLASSIFICAZIONE DEL TESTO e ADDESTRAMENTO MODELLO"],"metadata":{"id":"CjpAi8K-a6SH"},"id":"CjpAi8K-a6SH"},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","\n","#carico dataset\n","url = \"https://raw.githubusercontent.com/EliaVenturini/professionai-projects/main/spam_dataset.csv\"\n","X = pd.read_csv(url)\n","y = X['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","#Applico data_cleaner\n","X_train['text'] = X_train['text'].apply(data_cleaner)\n","X_test['text'] = X_test['text'].apply(data_cleaner)\n","\n","#Converte le righe di testo in vettori numerici usando avg_vector\n","X_train_vectors = np.array([avg_vector(sentence.split()) for sentence in X_train['text']])\n","X_test_vectors = np.array([avg_vector(sentence.split()) for sentence in X_test['text']])\n","#sentence.split() perch√® la funzione avg_vector() accetta una lista iterando cos√¨\n","#su ogni parola della frase\n","\n","\n","clf = MLPClassifier(activation='logistic',\n","                    hidden_layer_sizes=(100,),\n","                    max_iter=100,\n","                    solver='adam',\n","                    tol=0.005,\n","                    verbose=True)\n","\n","clf.fit(X_train_vectors, y_train)\n","#Il codice crea e allena una rete neurale di classificazione con un livello nascosto di 100 neuroni.\n","#Utilizza la funzione di attivazione logistica, l'ottimizzatore adam, e interrompe l'addestramento\n","#se non ci sono miglioramenti significativi nella perdita.\n","\n","#Rete neurale di classificazione (MLPClassifier):\n","# (MLPClassifier) √® un modello di classificazione che viene addestrato per distinguere due classi, come ad esempio spam e non-spam (detto anche ham).\n","#Una volta addestrato il modello con dati di esempio, questo modello pu√≤ classificare nuove email in spam o non-spam."],"metadata":{"id":"PALrPfWrbIrK","executionInfo":{"status":"ok","timestamp":1771927468745,"user_tz":-60,"elapsed":233901,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"colab":{"base_uri":"https://localhost:8080/","height":609},"outputId":"cf394265-7c41-4075-dbd8-33ab103d5f0c"},"id":"PALrPfWrbIrK","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.58260815\n","Iteration 2, loss = 0.52060436\n","Iteration 3, loss = 0.46206453\n","Iteration 4, loss = 0.40002664\n","Iteration 5, loss = 0.34142753\n","Iteration 6, loss = 0.29146758\n","Iteration 7, loss = 0.25309841\n","Iteration 8, loss = 0.22536051\n","Iteration 9, loss = 0.20410295\n","Iteration 10, loss = 0.18761112\n","Iteration 11, loss = 0.17435549\n","Iteration 12, loss = 0.16376060\n","Iteration 13, loss = 0.15393076\n","Iteration 14, loss = 0.14824484\n","Iteration 15, loss = 0.13994793\n","Iteration 16, loss = 0.13415294\n","Iteration 17, loss = 0.12856004\n","Iteration 18, loss = 0.12388906\n","Iteration 19, loss = 0.12021917\n","Iteration 20, loss = 0.11613106\n","Iteration 21, loss = 0.11279436\n","Iteration 22, loss = 0.11043640\n","Iteration 23, loss = 0.10731991\n","Iteration 24, loss = 0.10470676\n","Iteration 25, loss = 0.10230947\n","Iteration 26, loss = 0.10021856\n","Iteration 27, loss = 0.09815415\n","Iteration 28, loss = 0.09643895\n","Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n"]},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='logistic', max_iter=100, tol=0.005, verbose=True)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"‚ñ∏\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"‚ñæ\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, max_iter=100, tol=0.005, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, max_iter=100, tol=0.005, verbose=True)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# accuracy, precision, recall e F1-score\n","from sklearn.metrics import classification_report\n","\n","y_pred = clf.predict(X_test_vectors)\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zTdpx6vDQDR","executionInfo":{"status":"ok","timestamp":1771928485019,"user_tz":-60,"elapsed":45,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"outputId":"a6b43088-a8d7-4bb0-aa0d-8fcce2461a88"},"id":"4zTdpx6vDQDR","execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         ham       0.98      0.98      0.98       742\n","        spam       0.94      0.94      0.94       293\n","\n","    accuracy                           0.97      1035\n","   macro avg       0.96      0.96      0.96      1035\n","weighted avg       0.97      0.97      0.97      1035\n","\n"]}]},{"cell_type":"markdown","source":["APPLICARE IL MODELLO A NUOVE EMAIL"],"metadata":{"id":"o-olwTO1VRo_"},"id":"o-olwTO1VRo_"},{"cell_type":"code","source":["# Nuove email da classificare\n","new_emails = [\n","    \"Hi, we have an amazing offer just for you! Click here to win big prizes!\",\n","    \"Please find the project report attached. Let me know if there are any questions.\",\n","    \"Your account has been selected for a special reward. Act now to claim it.\"\n","]\n","\n","# 1. Pulizia delle nuove email\n","new_emails_cleaned = [data_cleaner(email) for email in new_emails]\n","\n","# 2. Trasforma le nuove email in vettori\n","new_emails_vectors = np.array([avg_vector(sentence.split()) for sentence in new_emails_cleaned])\n","\n","# 3. Classifica le nuove email utilizzando il modello allenato\n","predictions = clf.predict(new_emails_vectors)\n","\n","# 4. Interpreta i risultati\n","for email, label in zip(new_emails, predictions):\n","    if label == 'spam':\n","        print(f\"Email: {email}\\nClassificazione: SPAM\\n\")\n","    else:\n","        print(f\"Email: {email}\\nClassificazione: NON-SPAM\\n\")\n","#zip(new_emails, predictions): Questa funzione \"zippa\" (accoppia) insieme gli elementi di new_emails\n","#(le email da classificare) e predictions (le predizioni del modello, cio√® le etichette \"spam\" o \"non-spam\")."],"metadata":{"id":"RI83L1gXVUBZ","executionInfo":{"status":"ok","timestamp":1771928490175,"user_tz":-60,"elapsed":50,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4747d3fd-8881-4e9e-eae1-7446756a572e"},"id":"RI83L1gXVUBZ","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Email: Hi, we have an amazing offer just for you! Click here to win big prizes!\n","Classificazione: SPAM\n","\n","Email: Please find the project report attached. Let me know if there are any questions.\n","Classificazione: NON-SPAM\n","\n","Email: Your account has been selected for a special reward. Act now to claim it.\n","Classificazione: SPAM\n","\n"]}]},{"cell_type":"markdown","source":["ESTRAZIONE DELLE ORGANIZZAZIONE DALLE EMAIL NON SPAM"],"metadata":{"id":"w7LpxzajU1Wf"},"id":"w7LpxzajU1Wf"},{"cell_type":"code","source":["#Funzione per estrarre le organizzazioni\n","\n","import spacy   # Per l'analisi del linguaggio naturale (lemmatizzazione)\n","#SpaCy: per l'elaborazione del linguaggio naturale, in particolare per estrarre le entit√† come le organizzazioni.\n","nlp = spacy.load('en_core_web_sm') #carico il modello di Spacy per l'analisi del linguaggio (include lemmatizzazione)\n","\n","\n","def extract_organizations(text):\n","    doc = nlp(text)\n","    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n","    return organizations\n","\n","#Filtro le email non-SPAM\n","non_spam_emails = X[X['label'] == 'ham']['text']\n","non_spam_emails = non_spam_emails.apply(data_cleaner)\n","organizations = non_spam_emails.apply(extract_organizations)\n","\n","#Estrazione delle organizzazioni nelle email non-SPAM:\n","#(extract_organizations) serve per analizzare il contenuto delle email classificate come non-spam e individuare le organizzazioni menzionate al loro interno.\n","#Questo processo utilizza la libreria spaCy (o simile) per l'estrazione di entit√† (NER - Named Entity Recognition).\n","#La funzione extract_organizations estrae i nomi di entit√† etichettate come ORG (organizzazioni) dai testi.\n"],"metadata":{"id":"4bNDxjQNLRaQ","executionInfo":{"status":"ok","timestamp":1771928753401,"user_tz":-60,"elapsed":255991,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}}},"id":"4bNDxjQNLRaQ","execution_count":29,"outputs":[]},{"cell_type":"code","source":["# TOPIC MODELING SULLE EMAIL SPAM\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Filtro solo le email SPAM\n","spam_emails = X[X['label'] == 'spam']['text']\n","spam_emails_cleaned = spam_emails.apply(data_cleaner)\n","\n","# Vettorizzo il testo con CountVectorizer\n","vectorizer = CountVectorizer(max_features=1000, min_df=5)\n","X_spam_matrix = vectorizer.fit_transform(spam_emails_cleaned)\n","\n","# Addestro LDA con 5 topic\n","lda = LatentDirichletAllocation(n_components=5, random_state=42)\n","lda.fit(X_spam_matrix)\n","\n","# Mostro le top 10 parole per ogni topic\n","print(\" Topic principali nelle email SPAM:\\n\")\n","feature_names = vectorizer.get_feature_names_out()\n","for i, topic in enumerate(lda.components_):\n","    top_words = [feature_names[j] for j in topic.argsort()[-10:]]\n","    print(f\"Topic {i+1}: {', '.join(top_words)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Onylq09rE49B","executionInfo":{"status":"ok","timestamp":1771928829987,"user_tz":-60,"elapsed":76583,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"outputId":"c48efb73-fd72-4b27-8506-6505ca0f56fe"},"id":"Onylq09rE49B","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":[" Topic principali nelle email SPAM:\n","\n","Topic 1: online, time, www, new, want, good, com, get, http, subject\n","Topic 2: professional, microsoft, xp, office, subject, adobe, window, software, account, price\n","Topic 3: email, remove, please, subject, contact, www, message, nbsp, computron, com\n","Topic 4: inc, within, security, report, investment, may, information, stock, statement, company\n","Topic 5: color, tr, align, size, pill, width, height, http, td, font\n"]}]},{"cell_type":"code","source":["# TOPIC MODELING SULLE EMAIL SPAM\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Filtro solo le email SPAM\n","spam_emails = X[X['label'] == 'spam']['text']\n","spam_emails_cleaned = spam_emails.apply(data_cleaner)\n","\n","# Vettorizzo il testo con CountVectorizer\n","vectorizer = CountVectorizer(max_features=1000, min_df=5)\n","X_spam_matrix = vectorizer.fit_transform(spam_emails_cleaned)\n","\n","# Addestro LDA con 5 topic\n","lda = LatentDirichletAllocation(n_components=5, random_state=42)\n","lda.fit(X_spam_matrix)\n","\n","# Mostro le top 10 parole per ogni topic\n","print(\"Topic principali nelle email SPAM:\\n\")\n","feature_names = vectorizer.get_feature_names_out()\n","for i, topic in enumerate(lda.components_):\n","    top_words = [feature_names[j] for j in topic.argsort()[-10:]]\n","    print(f\"Topic {i+1}: {', '.join(top_words)}\")"],"metadata":{"id":"76fvwZvJXOKh","executionInfo":{"status":"ok","timestamp":1771928972223,"user_tz":-60,"elapsed":98286,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02c3a1f4-5802-4afc-d082-7f74714d50b3"},"id":"76fvwZvJXOKh","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Topic principali nelle email SPAM:\n","\n","Topic 1: online, time, www, new, want, good, com, get, http, subject\n","Topic 2: professional, microsoft, xp, office, subject, adobe, window, software, account, price\n","Topic 3: email, remove, please, subject, contact, www, message, nbsp, computron, com\n","Topic 4: inc, within, security, report, investment, may, information, stock, statement, company\n","Topic 5: color, tr, align, size, pill, width, height, http, td, font\n"]}]},{"cell_type":"code","source":["# DISTANZA SEMANTICA TRA I TOPIC\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","# Calcolo il vettore medio per ogni topic usando GloVe\n","topic_vectors = []\n","for i, topic in enumerate(lda.components_):\n","    top_words = [feature_names[j] for j in topic.argsort()[-10:]]\n","    topic_vector = avg_vector(top_words)\n","    topic_vectors.append(topic_vector)\n","\n","topic_vectors = np.array(topic_vectors)\n","\n","# Calcolo la similarit√† coseno tra i topic\n","similarity_matrix = cosine_similarity(topic_vectors)\n","\n","# Converto in distanza (1 - similarit√†)\n","distance_matrix = 1 - similarity_matrix\n","\n","print(\" Distanza semantica tra i topic (0=identici, 1=completamente diversi):\\n\")\n","for i in range(len(distance_matrix)):\n","    for j in range(i+1, len(distance_matrix)):\n","        print(f\"Topic {i+1} ‚Üî Topic {j+1}: {distance_matrix[i][j]:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNOEZZpGMOsX","executionInfo":{"status":"ok","timestamp":1771929011780,"user_tz":-60,"elapsed":27,"user":{"displayName":"ELIA VENTURINI","userId":"03736711909078589065"}},"outputId":"13e03b16-ecca-4ceb-a53e-d224b932a743"},"id":"kNOEZZpGMOsX","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":[" Distanza semantica tra i topic (0=identici, 1=completamente diversi):\n","\n","Topic 1 ‚Üî Topic 2: 0.395\n","Topic 1 ‚Üî Topic 3: 0.312\n","Topic 1 ‚Üî Topic 4: 0.411\n","Topic 1 ‚Üî Topic 5: 0.628\n","Topic 2 ‚Üî Topic 3: 0.584\n","Topic 2 ‚Üî Topic 4: 0.377\n","Topic 2 ‚Üî Topic 5: 0.686\n","Topic 3 ‚Üî Topic 4: 0.600\n","Topic 3 ‚Üî Topic 5: 0.733\n","Topic 4 ‚Üî Topic 5: 0.835\n"]}]},{"cell_type":"markdown","source":["## Conclusioni\n","\n","L'analisi della distanza semantica tra i 5 topic identificati nelle email SPAM\n","evidenzia una **buona eterogeneit√† complessiva** dei contenuti.\n","\n","Le coppie **Topic 3 ‚Üî Topic 5** (0.733) e **Topic 4 ‚Üî Topic 5** (0.835) mostrano\n","la maggiore distanza semantica, indicando che questi topic trattano argomenti\n","**molto diversi tra loro**.\n","\n","Al contrario, **Topic 1 ‚Üî Topic 3** (0.312) e **Topic 1 ‚Üî Topic 2** (0.395)\n","risultano pi√π simili, suggerendo una certa **sovrapposizione tematica**\n","probabilmente legata a pattern linguistici comuni nelle email SPAM\n","(es. offerte commerciali, premi, urgenza).\n","\n","In generale, il modello LDA ha identificato topic **sufficientemente distinti**\n","da confermare l'eterogeneit√† dei contenuti SPAM analizzati."],"metadata":{"id":"WbT8dfmgM8Uh"},"id":"WbT8dfmgM8Uh"},{"cell_type":"code","source":[],"metadata":{"id":"DJKqrOW3MwVy"},"id":"DJKqrOW3MwVy","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
